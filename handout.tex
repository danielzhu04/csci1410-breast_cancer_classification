\documentclass{article}
\usepackage{nicefrac}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=blue,
}
\usepackage{fullpage}

\title{CSCI1410 Fall 2023 \\
Assignment 6: Supervised Learning}

\date{Code Due Monday, November 20 at 11:59pm ET}

\begin{document}

\maketitle


\section{Goals}

The topic of this assignment is supervised learning, \emph{a.k.a.\/} function approximation.
There are two kinds of function approximation, classification and regression. You will first be implementing the k Nearest Neighbors algorithm for classification. Then, you will implement the perceptron algorithm, which you will use to solve both a classification and a regression problem.  You will be able to gauge the performance of your perceptron algorithm for regression against the TA's  linear least squares solution (implemented using Python's \texttt{sklearn} library) for regression.

\section{Introduction}

In supervised learning, you are given a data set of pairs,
where the first element of each pair is a feature $\bm{x} \in \mathbb{R}^{d}$,
and the second element is a label, $y \in R = \{ 0,1 \}$ in a (binary) classification problem,
or $y \in R = \mathbb{R}$ in a regression problem.
The underlying assumption is that those data represent a function $f: \mathbb{R}^{d} \to R$,
and the goal is to recover $f$ from the data.

A good predictor (classifier or regressor) is one which does not make too many mistakes, in the case of classification;
or does not make predictions that are too far off from the true label, in the case of regression.
In either case, it is not enough to tally errors on the training data alone: i.e., on all the points on which the predictor learns.
It is necessary to hold out some data, as a test set, to see whether the predictor can generalize well.
Analogously, it is not enough to test students on problems that were covered in class.
An exam asks students slightly different questions to gauge the depth of their understanding of the material.


\section{Algorithms}

There are numerous classification and regression techniques. In this assignment, you will work with k nearest neighbors for classification, and the perceptron for both classification and regression. The TA's have implemented a baseline least squares regression algorithm against which you can compare the performance of your perceptron.


The $k$NN algorithm is not really much of a learning algorithm at all,
as it does not have a training phase.
When queried with a data point $\bm{x}$,
it simply asks $\bm{x}$'s $k$ closest neighbors to vote on whether $\bm{x}$ should be classified as 0 or 1.
(The hyperparameter $k$ is usually chosen to be odd, to avoid ties.)
When $\bm{x} \in \mathbb{R}^d$, as is the case in this assignment,
``closeness'' can be measured using Euclidean distance.

Regression is also called \emph{data fitting}, since the goal is to fit a curve to the data.
The idea of linear least-squares regression is to fit a line to the data
such that the sum of the squared residuals (i.e., errors)
between the predictions and the labels (i.e., $\sum_{(\bm{x}, y)} (\hat{f}(\bm{x}) - y)^2$,
where $\hat{f}$ is the function learned by the regression algorithm) is minimal.
There exists a closed form solution to linear least-squares regression,
which involves inverting a (necessarily square) matrix,
an $O(n^3)$ operation, where $n$ is the dimension of the matrix.
This method is usually preferable in cases where matrix inversion is tractable.

Your main task in this assignment is to implement the perceptron algorithm.
Recall from lecture that perceptrons are feedforward neural networks.
A \emph{simple\/} perceptron is a feedforward neural network with only one layer:
i.e., all the inputs (i.e., features) feed into a single output layer.
More specifically, a weighted sum of all the inputs is fed into this output layer.
Perceptrons can function as classifiers or as regressors, depending on the form of the output layer.
When the outputs apply threshold functions (or smooth variants thereof: e.g., sigmoidals) to the weighted sum,
they function as classifiers;
alternatively, when the weighted sum itself is the output, they function as regressors.
Although the perceptron learning rule was originally developed for classification
(using a hard threshold function to compute the output),
it turns out that the exact same perceptron learning rule also applies to regression.
(See the lecture notes for details.)


\section{Data Sets}

\subsection{Breast Cancer Diagnostic Dataset}

This dataset contains features calculated from a digital image of a fine needle aspirate of a breast mass,
and a label representing the diagnosis, i.e., benign or malignant.
There are 569 data points and 31 features, which consist of the patient's ID
together with the mean, standard error, and  ``worst'' (i.e., the mean of the three largest values)
of ten measurements, such as radius, perimeter, area, etc.
%
Your goal is to use supervised learning to predict the classification label: i.e., benign or malignant. You will test your kNN classifier and perceptron classifier on this dataset.
You can find out more about the data
\href{http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)}{here}.
Please also refer to the file
\verb|breast_cancer_diagnostic_description.txt|.

\subsection{Student Achievement Dataset}

This dataset is a record of student achievement in 2008 at a secondary education at a Portuguese school.
Each student is represented by a feature vector that summarizes their demographic, social, and school-related characteristics.
%
Your goal is to use supervised learning to predict the regression label: i.e., a student's 3rd marking period grade. You will test your perceptron regression algorithm on this dataset.
You can find out more about the data
\href{http://archive.ics.uci.edu/ml/datasets/Student+Performance}{here}.
Please also refer to the file \verb|student_dataset_description.txt|.

\subsection{Data Files}

These two datasets are available in \texttt{.csv} files, with accompanying descriptions in \texttt{.txt} files.
Additionally, we are releasing \verb|preprocess_student_data.py|, python code which ``cleans'' the student dataset
(i.e., morphs it into a more readily usable form).

\if 0
\begin{itemize}
\item \verb|breast_cancer_diagnostic.csv| contains the Breast Cancer Diagnostic dataset.
\item \verb|breast_cancer_diagnostic.txt| describes the features in the Breast Cancer Diagnostic dataset.

\item \verb|student_dataset_description.txt| describes the features in the Student dataset.
\item \verb|student-por.csv| contains the Student dataset.
\item \verb|preprocess_student_data.py| cleans the student dataset.
\end{itemize}
\fi


\section{Stencil Code}
The stencil code for this assignment consists of eight files.
Below we provide a high-level overview of each.
Read the docstrings and comments for more detailed information.

\begin{itemize}
\item \verb|classification.py| imports \verb|breast_cancer_diagnostic.csv|
  and then classifies this dataset using a \verb|KNNClassifier| or a \verb|Perceptron|.

\item \verb|regression.py| imports \verb|student_por.csv| and then regresses
  on this dataset using \verb|SKLearnModel| or a \verb|Perceptron|.

\item \verb|knn.py| contains the \verb|KNNClassifier| class,
  which implements the $k$NN algorithm for classification.

  \textbf{Note:} In the stencil code, we use the phrase \emph{anchor points} to describe the nearest neighbors.
A $k$NN classifier classifies a data point $x$ by taking a plurality vote among $k$ anchor points,
namely those $k$ points that are closest in feature space to $x$.

\item \verb|perceptron.py| contains the \verb|Perceptron| class,
  for which you will implement \verb|train|, \verb|predict| and \verb|evaluate|.
  \textbf{You must implement a perceptron for both classification and regression.}

\item \verb|supervisedlearner.py| contains \verb|SupervisedLearner|,
  the abstract superclass of the other supervised learning classes for
  this assignment, namely \verb|KNNClassifier|, \verb|SKLearnModel|, and \verb|Perceptron|.

\item \verb|cross_validate.py| splits the data into train and test sets for cross validation.
  You should be sure to cross validate both your classification and regression models.
\end{itemize}

\if 0
At the top of the learning hierarchy for this assignment sits the abstract \verb|SupervisedLearner| class.
Each particular supervised learning model is then a child of this superclass.
Each supervised learning model therefore inherits several fields and methods,
including the abstract methods \verb|train|, \verb|predict|, and \verb|evaluate|,
which you will need to fill in for your implementation of the \verb|Perceptron| class in \verb|perceptron.py|.
\fi

For this assignment, you should edit only the following files:

\begin{itemize}
\item \verb|perceptron.py| - implement the \verb|Perceptron| class for both classification and regression by filling in
  \verb|step_function|, as well as the abstract methods \verb|train|, \verb|predict|, and \verb|evaluate|.

\item \verb|knn.py| - implement the kNN algorithm, filling in \verb|train|, \verb|predict|, and \verb|evaluate|. When training, you will need to compute the features of each anchor point using the corresponding feature function.  \textbf{Note:} You should aim for a KNN model accuracy of about 91\% when running classification with your KNN implementation [\verb|python classification.py -k|].

\item \verb|classification.py| - classify the Breast Cancer Diagnostic dataset using your \verb|Perceptron| model.
  Adjust the \verb|learning_rate| to optimize performance: i.e., the accuracy during cross validation,
  where accuracy is the number of \emph{correct\/} predictions divided by the total number of predictions.

  Also implement the \verb|optimal_k| method, which optimizes the value of $k$: i.e., the number of neighbors.
  \emph{Be sure $k$ is set to the best value you find when you submit.}

\item \verb|regression.py| - perform regression on the Student dataset using your \verb|Perceptron| model.
  Adjust the \verb|learning_rate| to optimize performance: i.e., the mean squared error---the
  residual sum of squares divided by the size of the dataset---during cross validation.
\end{itemize}

  Another way to gauge your performance in this regression task is to measure your accuracy,
  which for the Student dataset can be defined as 1 less the mean squared error divided by 20
  (20 is the normalizing factor because [0,20] is the range of the response variable).
  So, if your mean squared error is 1.5, your accuracy is $1 - \nicefrac{1.5}{20} = 92.5\%$.)
  
\textbf{Note:} You will find that you achieve different degrees of accuracy
depending on the \verb|train_size| in \verb|classification.py| and \verb|regression.py|:
i.e., the percentage of the dataset you use for training.
Feel free to experiment with different values of \verb|train_size| as well to optimize model performance.


\section{Running Your Code}

Here is a list of commands to run various algorithms that you implement:

\begin{itemize}
\item Running the KNN Classifier - [\verb|python classification.py -k|]
\item Running the Perceptron Classifier - [\verb|python classification.py|]
\item Running the Perceptron Regression - [\verb|python regression.py|]
    \begin{itemize}
        \item You can compare your Perceptron regression's performance against the TA's SKlearn implementation of linear least squares solution for regression by running [\verb|python regression.py -s|]
    \end{itemize}
\end{itemize}

% \section{Written Questions}
% Answer the following questions in clearly labeled sections.

% \begin{enumerate}
% \item Recall that a single-layer perceptrons can implement the Boolean functions AND and OR.

% \begin{enumerate}
% \item Design a single-layer, two-input perceptron that implements the Boolean function $X \wedge \neg Y$.
%   By ``design,'' we mean derive a set of weights and a bias term that implements this function.

% \item Design a two-layer, two-input perceptron that implements the XOR function. \\
%   \textbf{Hint:} Make use of part (a) of this question.
% \end{enumerate}

% \item Consider a two-dimensional binary classification problem,
%   in which all points inside the circle of radius 1 are labelled blue,
%   while all points outside the same circle are labelled red.


% \begin{enumerate}
% \item Are these classes linearly separable on the Cartesian plane?

% \item Are these classes linearly separable in some derived feature space:
%   i.e., after transforming the feature vectors into some higher-dimensional space by applying some basis functions?
%   Explain.
% \end{enumerate}

% \item List all the hyperparameters in your implementation of the perceptron algorithm,
%   and briefly summarize how you optimized them.
%   \emph{Be sure all these values are set to the best values you find when you submit.}
% \end{enumerate}


\section{Grading}

The Breast Cancer Diagnostic dataset is fairly easy to classify correctly.
You will earn full points by achieving 95\% accuracy on this dataset with your perceptron classification algorithm, and 91\% with your kNN classifier.

The Student dataset is trickier.
You will earn full points by achieving 85\% accuracy on this dataset using your perceptron regression algorithm.
However, you can earn extra credit by achieving 90\% accuracy or above on this dataset.
If you achieve 90\% accuracy, please report your findings on Ed so that other students can try to match your results.

\textbf{Note:}
You can can complete this assignment using only the ``raw'' feature values (normalized, most likely).
However, you may also wish to experiment with using ``derived'' feature values,
by which we mean feature values transformed by basis functions.
You may be able to increase your accuracy in this way.


\section{Install and Handin Instructions}
To install, accept the \href{https://classroom.github.com/a/K0iVrEtu}{GitHub Classroom assignment}. This will create a private GitHub repository with the stencil code for you to work on the
assignment. \\

To handin:
\begin{enumerate}
  \item Make sure to push any changes you want to test to your private
    repository. You can do this by running
    \begin{verbatim}
    git add .
    git commit -m "<a commit message describing what you changed>"
    git push
    \end{verbatim}

  \item On Gradescope, click on the assignment you are submitting for.

  \item Under ``Submission Method", please select GitHub.

  \item Under ``Repository", you can search for your repository by typing ``csci-1410"
    and selecting the repository for this assignment.

  \item Under ``Branch", you can select any branch that you want to be graded. So if
    you're testing something on a branch, you can see how its functionality
    performs here, before merging it to master. Feel free to upload your assignment
    as many times as you like before the deadline.
\end{enumerate}

% In accordance with the course grading policy, your written homework should not have any identifiable information on it, including your banner ID, name, or CS login.

\end{document}

